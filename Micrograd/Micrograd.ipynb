{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700d3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: graphviz in /opt/anaconda3/lib/python3.12/site-packages (0.20.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92c1331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5af73e",
   "metadata": {},
   "source": [
    "Engine of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10aa1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__ (self, data, _children=(), _op='', label=''):\n",
    "        \n",
    "        self.data = data # data \n",
    "        self._op = _op #store the operation (like +,- etc)\n",
    "        self._prev = set(_children) # where this came from \n",
    "        self.grad = 0.0 # default value \n",
    "        self._backward = lambda : None # defalut not activated \n",
    "        self.label = label # label for the each data point \n",
    "\n",
    "    def __add__(self, other):\n",
    "\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "\n",
    "        out = Tensor(self.data + other.data,(self,other),'+') # the other access the other data other than \n",
    "                                                              # the self.data = a , other.data = b \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad \n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward                                                     \n",
    "\n",
    "        return out \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other) # if the other object is not tensor then it converts the scalar to tensor\n",
    "\n",
    "        out = Tensor(self.data * other.data,(self,other),'*')\n",
    "\n",
    "        def _backward():\n",
    "\n",
    "            self.grad += other.data * out.grad # L = d * F - > F for mul der \n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "\n",
    "        return out \n",
    "    \n",
    "    def tanh(self):\n",
    "           x = self.data \n",
    "           \n",
    "           t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "           out = Tensor(t, (self, ), 'tanh')\n",
    "           \n",
    "           def _backward():\n",
    "               \n",
    "               self.grad += (1 - t**2) * out.grad\n",
    "               \n",
    "           out._backward = _backward\n",
    "           \n",
    "           return out \n",
    "      \n",
    "    def relu(self): # relu \n",
    "        x = self.data\n",
    "        t = np.maximum(0, x)\n",
    "        out = Tensor(t, (self,), 'relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (t > 0) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out \n",
    "    \n",
    "    def gelu(self): # Gelu \n",
    "        x = self.data\n",
    "        t = 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "        out = Tensor(t, (self,), 'gelu')\n",
    "\n",
    "        def _backward():\n",
    "            tanh_out = np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3)))\n",
    "            derivative = 0.5 * (1 + tanh_out + x * (1 - np.square(tanh_out)) * (np.sqrt(2 / np.pi) + 0.134145 * np.power(x, 2)))\n",
    "            self.grad += derivative * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self):  # sigmoid \n",
    "        x = self.data \n",
    "        t = 1 / (1 + np.exp(-x))\n",
    "        out = Tensor(t, (self,), 'sigmoid')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += t * (1 - t) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def softmax(self):  # softmax \n",
    "        x = self.data\n",
    "        exps = np.exp(x - np.max(x))\n",
    "        t = exps / np.sum(exps)\n",
    "        out = Tensor(t, (self,), 'softmax')\n",
    "\n",
    "        def _backward():\n",
    "            for i in range(len(t)):\n",
    "                self.grad[i] += t[i] * (1 - t[i]) * out.grad[i]\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def __rmul__(self,other): # arranging the a * b == b * a \n",
    "\n",
    "        return  self * other \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Tensor(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "    \n",
    "      topo = [] #  list to store nodes in topo order\n",
    "      visited = set() # Set to track visited nodes\n",
    "      \n",
    "      # Helper function to build topological order\n",
    "      def build_topo(v):\n",
    "        if v not in visited:\n",
    "          visited.add(v)\n",
    "          for child in v._prev: # ass : _prev contains parent nodes\n",
    "            build_topo(child)\n",
    "          topo.append(v)\n",
    "          \n",
    "      build_topo(self) # start building topo order from the current node\n",
    "     # print(topo)\n",
    "      # print(visited)\n",
    "      \n",
    "      self.grad = 1.0 # initialize the gradient of the final node (self) | so it can avoid zero curse \n",
    "      for node in reversed(topo):  # backward pass in topological order\n",
    "        node._backward()\n",
    "        \n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "    \n",
    "    def __repr__(self):\n",
    "\n",
    "        return f\"Tensor(data={self.data})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49837798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=-8.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor(2.0, label='a')\n",
    "b = Tensor(-3.0, label='b')\n",
    "c = Tensor(10.0, label='c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Tensor(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4516b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147eb52",
   "metadata": {},
   "source": [
    "Now Lets build the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f35d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Tensor(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        self.b = Tensor(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd7c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
